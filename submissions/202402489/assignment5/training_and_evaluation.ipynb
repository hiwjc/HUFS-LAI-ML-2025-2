{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IINOHSlQfdyI"
   },
   "source": [
    "train, test, val dataë¥¼ ì½”ë“œë¥¼ ì´ìš©í•˜ì—¬ êµ¬ë¶„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYtWH_SZT0ln"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# ========================================================================================\n",
    "# [ì„¤ì •] íŒŒì¼ ê²½ë¡œ ë° ë¹„ìœ¨\n",
    "# ========================================================================================\n",
    "CONFIG = {\n",
    "    'input_file': 'í†µí•©_ë¦¬ë·°_ê¸°ì—…ì¤‘ë³µì œê±°.xlsx - Sheet1.csv', # [ìˆ˜ì •] ì—…ë¡œë“œëœ CSV íŒŒì¼ëª…ìœ¼ë¡œ ë³€ê²½\n",
    "    'train_ratio': 0.8,\n",
    "    'val_ratio': 0.1,\n",
    "    'test_ratio': 0.1,\n",
    "    'seed': 42 # ëœë¤ ì‹œë“œ ê³ ì • (ì–¸ì œ ì‹¤í–‰í•´ë„ ë˜‘ê°™ì´ ì„ì´ë„ë¡)\n",
    "}\n",
    "\n",
    "def split_and_save_dataset():\n",
    "    # 1. ì›ë³¸ ë°ì´í„° ë¡œë“œ\n",
    "    if not os.path.exists(CONFIG['input_file']):\n",
    "        print(f\"âŒ ì˜¤ë¥˜: '{CONFIG['input_file']}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"   (ì½”ë©ì˜ ì¢Œì¸¡ íŒŒì¼ íƒ­ì— íŒŒì¼ì„ ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.)\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ“‚ ì›ë³¸ ë°ì´í„° ë¡œë”© ì¤‘... ({CONFIG['input_file']})\")\n",
    "\n",
    "    try:\n",
    "        # [ìˆ˜ì •] read_excel -> read_csv ë³€ê²½ (CSV íŒŒì¼ì´ë¯€ë¡œ)\n",
    "        df = pd.read_csv(CONFIG['input_file'])\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ íŒŒì¼ì„ ì½ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "        return\n",
    "\n",
    "    total_len = len(df)\n",
    "    print(f\" -> ì „ì²´ ë°ì´í„° ìˆ˜: {total_len}ê°œ\")\n",
    "\n",
    "    # 2. ë°ì´í„° ì…”í”Œ ë° ë¶„í•  (8:1:1)\n",
    "    # ë¨¼ì € Train(80%) vs Temp(20%)ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df,\n",
    "        test_size=(1 - CONFIG['train_ratio']),\n",
    "        random_state=CONFIG['seed'],\n",
    "        shuffle=True # ë¬´ì‘ìœ„ ì„ê¸° í•„ìˆ˜\n",
    "    )\n",
    "\n",
    "    # ë‚¨ì€ Temp(20%)ë¥¼ ì ˆë°˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ Val(10%) vs Test(10%)ë¡œ ë§Œë“­ë‹ˆë‹¤.\n",
    "    # (temp_df ë‚´ì—ì„œì˜ 0.5ëŠ” ì „ì²´ì˜ 0.1ì´ ë©ë‹ˆë‹¤)\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=0.5,\n",
    "        random_state=CONFIG['seed'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # 3. ë¶„í•  ê²°ê³¼ í™•ì¸\n",
    "    print(\"\\nğŸ“Š ë°ì´í„° ë¶„í•  ê²°ê³¼\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"1ï¸âƒ£ Train Set (í•™ìŠµ) : {len(train_df)}ê°œ ({len(train_df)/total_len*100:.1f}%)\")\n",
    "    print(f\"2ï¸âƒ£ Valid Set (ê²€ì¦) : {len(val_df)}ê°œ ({len(val_df)/total_len*100:.1f}%)\")\n",
    "    print(f\"3ï¸âƒ£ Test Set  (í‰ê°€) : {len(test_df)}ê°œ ({len(test_df)/total_len*100:.1f}%)\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"í•©ê³„ : {len(train_df) + len(val_df) + len(test_df)}ê°œ\")\n",
    "\n",
    "    # 4. íŒŒì¼ë¡œ ì €ì¥\n",
    "    print(\"\\nğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘...\")\n",
    "    # í•™ìŠµ/í‰ê°€ìš© ë°ì´í„°ëŠ” í˜¸í™˜ì„±ì´ ì¢‹ì€ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    train_df.to_csv('train.csv', index=False, encoding='utf-8-sig')\n",
    "    val_df.to_csv('val.csv', index=False, encoding='utf-8-sig')\n",
    "    test_df.to_csv('test.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(\"âœ… ì €ì¥ ì™„ë£Œ! ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:\")\n",
    "    print(\"   - train.csv\")\n",
    "    print(\"   - val.csv\")\n",
    "    print(\"   - test.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    split_and_save_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyZeYJ3wfpK5"
   },
   "source": [
    "êµ¬ë¶„í•œ ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµ ë° í‰ê°€ì˜ ê³¼ì •ì„ ê±°ì³ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ë¥¼ í™•ì¸í•˜ë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8f6TWsuCbXZi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import pandas as pd # pandas ì¶”ê°€\n",
    "from torch.utils.data import Dataset, DataLoader # Dataset, DataLoader ì¶”ê°€\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import os\n",
    "\n",
    "# ========================================================================================\n",
    "# [Data Loader Section] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í´ë˜ìŠ¤ (í†µí•©ë¨)\n",
    "# ========================================================================================\n",
    "\n",
    "# ë°ì´í„° ì„¤ì • (data_loader.py ë‚´ìš©ì„ ì—¬ê¸°ë¡œ ê°€ì ¸ì˜´)\n",
    "DATA_CONFIG = {\n",
    "    'model_name': 'klue/bert-base',\n",
    "    'max_len': 128,\n",
    "    'batch_size': 16,\n",
    "    'train_file': 'train.csv',\n",
    "    'val_file': 'val.csv',\n",
    "    'test_file': 'test.csv',\n",
    "}\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'review_text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"CSV ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
    "    df.columns = [str(col).strip() for col in df.columns]\n",
    "\n",
    "    rename_map = {\n",
    "        'êµ¬ë¶„': 'job_role', 'ê¸°ì—…ëª…': 'company',\n",
    "        'ë³µì§€ ë° ê¸‰ì—¬': 'salary', 'ì—…ë¬´ì™€ ì‚¶ì˜ ê· í˜•': 'wlb', 'ì‚¬ë‚´ë¬¸í™”': 'culture',\n",
    "        'ê²½ì˜ì§„': 'leadership', 'ìŠ¹ì§„ ê¸°íšŒ': 'promotion', 'ì¢…í•©í‰ì ': 'overall'\n",
    "    }\n",
    "    available_cols = set(df.columns)\n",
    "    valid_rename = {k:v for k,v in rename_map.items() if k in available_cols}\n",
    "    df = df.rename(columns=valid_rename)\n",
    "\n",
    "    df['full_text'] = (\n",
    "        df['ì¥ì '].fillna('').astype(str) + \" \" +\n",
    "        df['ë‹¨ì '].fillna('').astype(str) + \" \" +\n",
    "        df['ìš”ì•½'].fillna('').astype(str)\n",
    "    )\n",
    "\n",
    "    target_cols = ['salary', 'wlb', 'culture', 'leadership', 'promotion', 'overall']\n",
    "    for col in target_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(3.0)\n",
    "\n",
    "    valid_target_cols = [col for col in target_cols if col in df.columns]\n",
    "    return df, valid_target_cols\n",
    "\n",
    "def get_data_loaders():\n",
    "    \"\"\"ë°ì´í„°ì…‹ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ DataLoader ìƒì„±\"\"\"\n",
    "    print(\"\\n[Data Loader] CSV íŒŒì¼ ë¡œë“œ ì‹œì‘...\")\n",
    "\n",
    "    # íŒŒì¼ ê²½ë¡œ í™•ì¸\n",
    "    files = {\n",
    "        'Train': DATA_CONFIG['train_file'],\n",
    "        'Val': DATA_CONFIG['val_file'],\n",
    "        'Test': DATA_CONFIG['test_file']\n",
    "    }\n",
    "\n",
    "    for name, path in files.items():\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"âŒ '{path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. dataset_splitter.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "    df_train = pd.read_csv(files['Train'])\n",
    "    df_val = pd.read_csv(files['Val'])\n",
    "    df_test = pd.read_csv(files['Test'])\n",
    "\n",
    "    df_train, target_cols = preprocess_dataframe(df_train)\n",
    "    df_val, _ = preprocess_dataframe(df_val)\n",
    "    df_test, _ = preprocess_dataframe(df_test)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(DATA_CONFIG['model_name'])\n",
    "\n",
    "    def create_loader(df):\n",
    "        ds = ReviewDataset(\n",
    "            texts=df['full_text'].to_numpy(),\n",
    "            targets=df[target_cols].to_numpy(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_len=DATA_CONFIG['max_len']\n",
    "        )\n",
    "        return DataLoader(ds, batch_size=DATA_CONFIG['batch_size'], num_workers=0)\n",
    "\n",
    "    return create_loader(df_train), create_loader(df_val), create_loader(df_test), target_cols, tokenizer\n",
    "\n",
    "\n",
    "# ========================================================================================\n",
    "# [ì„¤ì •] í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "# ========================================================================================\n",
    "TRAIN_CONFIG = {\n",
    "    'epochs': 5,             # í•™ìŠµ ë°˜ë³µ íšŸìˆ˜\n",
    "    'learning_rate': 2e-5,   # í•™ìŠµë¥ \n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'model_save_path': 'review_rating_model.pt' # í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ\n",
    "}\n",
    "\n",
    "# ========================================================================================\n",
    "# [Model Architecture] BERT Regressor\n",
    "# ========================================================================================\n",
    "class ReviewRatingRegressor(nn.Module):\n",
    "    def __init__(self, n_targets=6):\n",
    "        super(ReviewRatingRegressor, self).__init__()\n",
    "        # DATA_CONFIGë¥¼ ì§ì ‘ ì°¸ì¡°\n",
    "        self.bert = AutoModel.from_pretrained(DATA_CONFIG['model_name'])\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_targets)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooler_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False\n",
    "        )\n",
    "        output = self.drop(pooler_output)\n",
    "        return self.out(output), pooler_output\n",
    "\n",
    "# ========================================================================================\n",
    "# [Training & Evaluation Steps]\n",
    "# ========================================================================================\n",
    "def train_step(model, data_loader, loss_fn, optimizer, scheduler, device):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "def eval_step(model, data_loader, loss_fn, device):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "# ========================================================================================\n",
    "# [Main Execution Flow]\n",
    "# ========================================================================================\n",
    "def run_training():\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸš€ ë¦¬ë·° í‰ì  ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # 1. ë°ì´í„° ë¡œë“œ (ë‚´ë¶€ í•¨ìˆ˜ í˜¸ì¶œ)\n",
    "    train_loader, val_loader, test_loader, target_cols, _ = get_data_loaders()\n",
    "\n",
    "    # 2. ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    device = TRAIN_CONFIG['device']\n",
    "    print(f\"\\n[Setup] Device: {device}\")\n",
    "    model = ReviewRatingRegressor(n_targets=len(target_cols)).to(device)\n",
    "\n",
    "    # 3. Optimizer & Scheduler ì„¤ì •\n",
    "    optimizer = AdamW(model.parameters(), lr=TRAIN_CONFIG['learning_rate'])\n",
    "    total_steps = len(train_loader) * TRAIN_CONFIG['epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    loss_fn = nn.MSELoss().to(device)\n",
    "\n",
    "    # 4. í•™ìŠµ ë£¨í”„ (Training Loop)\n",
    "    print(\"\\n[Training Process]\")\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(TRAIN_CONFIG['epochs']):\n",
    "        train_loss = train_step(model, train_loader, loss_fn, optimizer, scheduler, device)\n",
    "        val_loss = eval_step(model, val_loader, loss_fn, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{TRAIN_CONFIG['epochs']} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            torch.save(model.state_dict(), TRAIN_CONFIG['model_save_path'])\n",
    "            best_loss = val_loss\n",
    "            print(f\"   âœ… Best Model Saved! (Val Loss: {best_loss:.4f})\")\n",
    "\n",
    "    # 5. ìµœì¢… í‰ê°€ (Test Set Evaluation)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ† ìµœì¢… ëª¨ë¸ í‰ê°€ (Test Set)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if os.path.exists(TRAIN_CONFIG['model_save_path']):\n",
    "        model.load_state_dict(torch.load(TRAIN_CONFIG['model_save_path']))\n",
    "        print(\" -> í•™ìŠµ ì¤‘ ì €ì¥ëœ ìµœê³ ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\" -> ì €ì¥ëœ ëª¨ë¸ì´ ì—†ì–´ ë§ˆì§€ë§‰ ì—í¬í¬ ëª¨ë¸ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    preds, reals = [], []\n",
    "    with torch.no_grad():\n",
    "        for d in test_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs, _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            reals.extend(targets.cpu().numpy())\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    reals = np.array(reals)\n",
    "\n",
    "    mae = mean_absolute_error(reals, preds)\n",
    "    r2 = r2_score(reals, preds)\n",
    "\n",
    "    print(f\"\\nğŸ“Š Test ê²°ê³¼ ìš”ì•½:\")\n",
    "    print(f\"   - MAE (í‰ê·  ì˜¤ì°¨) : {mae:.4f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)\")\n",
    "    print(f\"   - R2 Score (ê²°ì •ê³„ìˆ˜): {r2:.4f} (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)\")\n",
    "\n",
    "    print(\"\\n[ğŸ’¡ í•´ì„]\")\n",
    "    print(f\"   ì´ ëª¨ë¸ì€ ì‚¬ìš©ìì˜ ë¦¬ë·° í…ìŠ¤íŠ¸ë§Œ ë³´ê³  ì‹¤ì œ í‰ì ê³¼ í‰ê·  {mae:.2f}ì  ì°¨ì´ë¡œ ì˜ˆì¸¡í•˜ê³  ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
