# í•™ìŠµí• ë•Œ ì‚¬ìš©í•œ ë¦¬ë·° íŒŒì¼ì„ í›ˆë ¨ ë°ì´í„°ì…‹ê³¼ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³ , í•™ìŠµí•˜ëŠ” ê³¼ì •ì´ ë‹´ê¸´ ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. í‰ê°€ì—ì„œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ê°€ì§„ ëª¨ë¸ì„ ê°€ì ¸ì˜¨ ë’¤ ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œ inferenceë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤.

import pandas as pd
from sklearn.model_selection import train_test_split
import os

# ========================================================================================
# [ì„¤ì •] íŒŒì¼ ê²½ë¡œ ë° ë¹„ìœ¨
# ========================================================================================
CONFIG = {
    'input_file': 'í†µí•©_ë¦¬ë·°_ê¸°ì—…ì¤‘ë³µì œê±°.xlsx', # ì—‘ì…€ íŒŒì¼ëª…ìœ¼ë¡œ ìˆ˜ì •
    'train_ratio': 0.8,
    'val_ratio': 0.1,
    'test_ratio': 0.1,
    'seed': 42 # ëœë¤ ì‹œë“œ ê³ ì • (ì–¸ì œ ì‹¤í–‰í•´ë„ ë˜‘ê°™ì´ ì„ì´ë„ë¡)
}

def split_and_save_dataset():
    # 1. ì›ë³¸ ë°ì´í„° ë¡œë“œ
    if not os.path.exists(CONFIG['input_file']):
        print(f"âŒ ì˜¤ë¥˜: '{CONFIG['input_file']}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   (ì½”ë©ì˜ ì¢Œì¸¡ íŒŒì¼ íƒ­ì— ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.)")
        return

    print(f"ğŸ“‚ ì›ë³¸ ë°ì´í„° ë¡œë”© ì¤‘... ({CONFIG['input_file']})")

    try:
        # [ìˆ˜ì •] read_csv -> read_excel ë³€ê²½
        df = pd.read_excel(CONFIG['input_file'])
    except Exception as e:
        print(f"âŒ ì—‘ì…€ íŒŒì¼ì„ ì½ëŠ” ë„ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return

    total_len = len(df)
    print(f" -> ì „ì²´ ë°ì´í„° ìˆ˜: {total_len}ê°œ")

    # 2. ë°ì´í„° ì…”í”Œ ë° ë¶„í•  (8:1:1)
    # ë¨¼ì € Train(80%) vs Temp(20%)ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    train_df, temp_df = train_test_split(
        df,
        test_size=(1 - CONFIG['train_ratio']),
        random_state=CONFIG['seed'],
        shuffle=True # ë¬´ì‘ìœ„ ì„ê¸° í•„ìˆ˜
    )

    # ë‚¨ì€ Temp(20%)ë¥¼ ì ˆë°˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ Val(10%) vs Test(10%)ë¡œ ë§Œë“­ë‹ˆë‹¤.
    # (temp_df ë‚´ì—ì„œì˜ 0.5ëŠ” ì „ì²´ì˜ 0.1ì´ ë©ë‹ˆë‹¤)
    val_df, test_df = train_test_split(
        temp_df,
        test_size=0.5,
        random_state=CONFIG['seed'],
        shuffle=True
    )

    # 3. ë¶„í•  ê²°ê³¼ í™•ì¸
    print("\nğŸ“Š ë°ì´í„° ë¶„í•  ê²°ê³¼")
    print("-" * 30)
    print(f"1ï¸âƒ£ Train Set (í•™ìŠµ) : {len(train_df)}ê°œ ({len(train_df)/total_len*100:.1f}%)")
    print(f"2ï¸âƒ£ Valid Set (ê²€ì¦) : {len(val_df)}ê°œ ({len(val_df)/total_len*100:.1f}%)")
    print(f"3ï¸âƒ£ Test Set  (í‰ê°€) : {len(test_df)}ê°œ ({len(test_df)/total_len*100:.1f}%)")
    print("-" * 30)
    print(f"í•©ê³„ : {len(train_df) + len(val_df) + len(test_df)}ê°œ")

    # 4. íŒŒì¼ë¡œ ì €ì¥
    print("\nğŸ’¾ íŒŒì¼ ì €ì¥ ì¤‘...")
    # í•™ìŠµ/í‰ê°€ìš© ë°ì´í„°ëŠ” í˜¸í™˜ì„±ì´ ì¢‹ì€ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    train_df.to_csv('train.csv', index=False, encoding='utf-8-sig')
    val_df.to_csv('val.csv', index=False, encoding='utf-8-sig')
    test_df.to_csv('test.csv', index=False, encoding='utf-8-sig')

    print("âœ… ì €ì¥ ì™„ë£Œ! ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤:")
    print("   - train.csv")
    print("   - val.csv")
    print("   - test.csv")

if __name__ == "__main__":
    split_and_save_dataset()

###

import torch
import torch.nn as nn
from torch.optim import AdamW
from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup
import numpy as np
import pandas as pd # pandas ì¶”ê°€
from torch.utils.data import Dataset, DataLoader # Dataset, DataLoader ì¶”ê°€
from sklearn.metrics import mean_absolute_error, r2_score
import os

# ========================================================================================
# [Data Loader Section] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í´ë˜ìŠ¤ (í†µí•©ë¨)
# ========================================================================================

# ë°ì´í„° ì„¤ì • (data_loader.py ë‚´ìš©ì„ ì—¬ê¸°ë¡œ ê°€ì ¸ì˜´)
DATA_CONFIG = {
    'model_name': 'klue/bert-base',
    'max_len': 128,
    'batch_size': 16,
    'train_file': 'train.csv',
    'val_file': 'val.csv',
    'test_file': 'test.csv',
}

class ReviewDataset(Dataset):
    def __init__(self, texts, targets, tokenizer, max_len):
        self.texts = texts
        self.targets = targets
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, item):
        text = str(self.texts[item])
        target = self.targets[item]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        return {
            'review_text': text,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'targets': torch.tensor(target, dtype=torch.float)
        }

def preprocess_dataframe(df):
    """CSV ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    df.columns = [str(col).strip() for col in df.columns]

    rename_map = {
        'êµ¬ë¶„': 'job_role', 'ê¸°ì—…ëª…': 'company',
        'ë³µì§€ ë° ê¸‰ì—¬': 'salary', 'ì—…ë¬´ì™€ ì‚¶ì˜ ê· í˜•': 'wlb', 'ì‚¬ë‚´ë¬¸í™”': 'culture',
        'ê²½ì˜ì§„': 'leadership', 'ìŠ¹ì§„ ê¸°íšŒ': 'promotion', 'ì¢…í•©í‰ì ': 'overall'
    }
    available_cols = set(df.columns)
    valid_rename = {k:v for k,v in rename_map.items() if k in available_cols}
    df = df.rename(columns=valid_rename)

    df['full_text'] = (
        df['ì¥ì '].fillna('').astype(str) + " " +
        df['ë‹¨ì '].fillna('').astype(str) + " " +
        df['ìš”ì•½'].fillna('').astype(str)
    )

    target_cols = ['salary', 'wlb', 'culture', 'leadership', 'promotion', 'overall']
    for col in target_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(3.0)

    valid_target_cols = [col for col in target_cols if col in df.columns]
    return df, valid_target_cols

def get_data_loaders():
    """ë°ì´í„°ì…‹ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ DataLoader ìƒì„±"""
    print("\n[Data Loader] CSV íŒŒì¼ ë¡œë“œ ì‹œì‘...")

    # íŒŒì¼ ê²½ë¡œ í™•ì¸
    files = {
        'Train': DATA_CONFIG['train_file'],
        'Val': DATA_CONFIG['val_file'],
        'Test': DATA_CONFIG['test_file']
    }

    for name, path in files.items():
        if not os.path.exists(path):
            raise FileNotFoundError(f"âŒ '{path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. dataset_splitter.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ê±°ë‚˜ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

    df_train = pd.read_csv(files['Train'])
    df_val = pd.read_csv(files['Val'])
    df_test = pd.read_csv(files['Test'])

    df_train, target_cols = preprocess_dataframe(df_train)
    df_val, _ = preprocess_dataframe(df_val)
    df_test, _ = preprocess_dataframe(df_test)

    tokenizer = AutoTokenizer.from_pretrained(DATA_CONFIG['model_name'])

    def create_loader(df):
        ds = ReviewDataset(
            texts=df['full_text'].to_numpy(),
            targets=df[target_cols].to_numpy(),
            tokenizer=tokenizer,
            max_len=DATA_CONFIG['max_len']
        )
        return DataLoader(ds, batch_size=DATA_CONFIG['batch_size'], num_workers=0)

    return create_loader(df_train), create_loader(df_val), create_loader(df_test), target_cols, tokenizer


# ========================================================================================
# [ì„¤ì •] í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ========================================================================================
TRAIN_CONFIG = {
    'epochs': 5,             # í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
    'learning_rate': 2e-5,   # í•™ìŠµë¥ 
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'model_save_path': 'review_rating_model.pt' # í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
}

# ========================================================================================
# [Model Architecture] BERT Regressor
# ========================================================================================
class ReviewRatingRegressor(nn.Module):
    def __init__(self, n_targets=6):
        super(ReviewRatingRegressor, self).__init__()
        # DATA_CONFIGë¥¼ ì§ì ‘ ì°¸ì¡°
        self.bert = AutoModel.from_pretrained(DATA_CONFIG['model_name'])
        self.drop = nn.Dropout(p=0.3)
        self.out = nn.Linear(self.bert.config.hidden_size, n_targets)

    def forward(self, input_ids, attention_mask):
        _, pooler_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=False
        )
        output = self.drop(pooler_output)
        return self.out(output), pooler_output

# ========================================================================================
# [Training & Evaluation Steps]
# ========================================================================================
def train_step(model, data_loader, loss_fn, optimizer, scheduler, device):
    model = model.train()
    losses = []

    for d in data_loader:
        input_ids = d["input_ids"].to(device)
        attention_mask = d["attention_mask"].to(device)
        targets = d["targets"].to(device)

        outputs, _ = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = loss_fn(outputs, targets)

        losses.append(loss.item())
        loss.backward()

        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    return np.mean(losses)

def eval_step(model, data_loader, loss_fn, device):
    model = model.eval()
    losses = []

    with torch.no_grad():
        for d in data_loader:
            input_ids = d["input_ids"].to(device)
            attention_mask = d["attention_mask"].to(device)
            targets = d["targets"].to(device)

            outputs, _ = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = loss_fn(outputs, targets)
            losses.append(loss.item())

    return np.mean(losses)

# ========================================================================================
# [Main Execution Flow]
# ========================================================================================
def run_training():
    print("="*60)
    print("ğŸš€ ë¦¬ë·° í‰ì  ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("="*60)

    # 1. ë°ì´í„° ë¡œë“œ (ë‚´ë¶€ í•¨ìˆ˜ í˜¸ì¶œ)
    train_loader, val_loader, test_loader, target_cols, _ = get_data_loaders()

    # 2. ëª¨ë¸ ì´ˆê¸°í™”
    device = TRAIN_CONFIG['device']
    print(f"\n[Setup] Device: {device}")
    model = ReviewRatingRegressor(n_targets=len(target_cols)).to(device)

    # 3. Optimizer & Scheduler ì„¤ì •
    optimizer = AdamW(model.parameters(), lr=TRAIN_CONFIG['learning_rate'])
    total_steps = len(train_loader) * TRAIN_CONFIG['epochs']
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

    loss_fn = nn.MSELoss().to(device)

    # 4. í•™ìŠµ ë£¨í”„ (Training Loop)
    print("\n[Training Process]")
    best_loss = float('inf')

    for epoch in range(TRAIN_CONFIG['epochs']):
        train_loss = train_step(model, train_loader, loss_fn, optimizer, scheduler, device)
        val_loss = eval_step(model, val_loader, loss_fn, device)

        print(f"Epoch {epoch+1}/{TRAIN_CONFIG['epochs']} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        if val_loss < best_loss:
            torch.save(model.state_dict(), TRAIN_CONFIG['model_save_path'])
            best_loss = val_loss
            print(f"   âœ… Best Model Saved! (Val Loss: {best_loss:.4f})")

    # 5. ìµœì¢… í‰ê°€ (Test Set Evaluation)
    print("\n" + "="*60)
    print("ğŸ† ìµœì¢… ëª¨ë¸ í‰ê°€ (Test Set)")
    print("="*60)

    if os.path.exists(TRAIN_CONFIG['model_save_path']):
        model.load_state_dict(torch.load(TRAIN_CONFIG['model_save_path']))
        print(" -> í•™ìŠµ ì¤‘ ì €ì¥ëœ ìµœê³ ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    else:
        print(" -> ì €ì¥ëœ ëª¨ë¸ì´ ì—†ì–´ ë§ˆì§€ë§‰ ì—í¬í¬ ëª¨ë¸ë¡œ í‰ê°€í•©ë‹ˆë‹¤.")

    model.eval()

    preds, reals = [], []
    with torch.no_grad():
        for d in test_loader:
            input_ids = d["input_ids"].to(device)
            attention_mask = d["attention_mask"].to(device)
            targets = d["targets"].to(device)

            outputs, _ = model(input_ids=input_ids, attention_mask=attention_mask)

            preds.extend(outputs.cpu().numpy())
            reals.extend(targets.cpu().numpy())

    preds = np.array(preds)
    reals = np.array(reals)

    mae = mean_absolute_error(reals, preds)
    r2 = r2_score(reals, preds)

    print(f"\nğŸ“Š Test ê²°ê³¼ ìš”ì•½:")
    print(f"   - MAE (í‰ê·  ì˜¤ì°¨) : {mae:.4f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    print(f"   - R2 Score (ê²°ì •ê³„ìˆ˜): {r2:.4f} (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)")

    print("\n[ğŸ’¡ í•´ì„]")
    print(f"   ì´ ëª¨ë¸ì€ ì‚¬ìš©ìì˜ ë¦¬ë·° í…ìŠ¤íŠ¸ë§Œ ë³´ê³  ì‹¤ì œ í‰ì ê³¼ í‰ê·  {mae:.2f}ì  ì°¨ì´ë¡œ ì˜ˆì¸¡í•˜ê³  ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_training()

# ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ë¡œ íŒë‹¨ë˜ëŠ” ëª¨ë¸ì— ëŒ€í•œ ì •ë³´ë¥¼ .pt íŒŒì¼ë¡œ ì €ì¥ë˜ì–´ ë‹¤ìŒ ìµœì¢… ì§ë¬´ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤ ì´ ê³¼ì •ì—ì„œ í•„ìš”í•œ ì›ë³¸ íŒŒì¼ì€ ë°ì´í„° ë¶„ì„ ë‹¨ê³„ì¸ ê³¼ì œ 3ì— ì²¨ë¶€ëœ ê¸°ì—… ë¦¬ë·° íŒŒì¼ì„ ì´ìš”í•˜ì‹œë©´ ë©ë‹ˆë‹¤.

import torch
import torch.nn as nn  # nn ëª¨ë“ˆ ì¶”ê°€
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel  # AutoModel ì¶”ê°€
import os

# ========================================================================================
# [Model Architecture] ë…ë¦½ ì‹¤í–‰ì„ ìœ„í•´ ëª¨ë¸ í´ë˜ìŠ¤ ì§ì ‘ ì •ì˜
# ========================================================================================
class ReviewRatingRegressor(nn.Module):
    def __init__(self, n_targets=6):
        super(ReviewRatingRegressor, self).__init__()
        self.bert = AutoModel.from_pretrained('klue/bert-base') # ëª¨ë¸ëª… í•˜ë“œì½”ë”© (í•™ìŠµê³¼ ë™ì¼)
        self.drop = nn.Dropout(p=0.3)
        self.out = nn.Linear(self.bert.config.hidden_size, n_targets)

    def forward(self, input_ids, attention_mask):
        _, pooler_output = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=False
        )
        output = self.drop(pooler_output)
        return self.out(output), pooler_output

# ========================================================================================
# [Preprocessing] ì „ì²˜ë¦¬ í•¨ìˆ˜ ì§ì ‘ ì •ì˜
# ========================================================================================
def local_preprocess_dataframe(df):
    # ì»¬ëŸ¼ëª… ê³µë°± ì œê±°
    df.columns = [str(col).strip() for col in df.columns]

    # ì»¬ëŸ¼ ë§¤í•‘ (í•œê¸€ -> ì˜ë¬¸)
    rename_map = {
        'êµ¬ë¶„': 'job_role', 'ê¸°ì—…ëª…': 'company',
        'ë³µì§€ ë° ê¸‰ì—¬': 'salary', 'ì—…ë¬´ì™€ ì‚¶ì˜ ê· í˜•': 'wlb', 'ì‚¬ë‚´ë¬¸í™”': 'culture',
        'ê²½ì˜ì§„': 'leadership', 'ìŠ¹ì§„ ê¸°íšŒ': 'promotion', 'ì¢…í•©í‰ì ': 'overall'
    }
    available_cols = set(df.columns)
    valid_rename = {k:v for k,v in rename_map.items() if k in available_cols}
    df = df.rename(columns=valid_rename)

    # í…ìŠ¤íŠ¸ ê²°í•© (ì¥ì  + ë‹¨ì  + ìš”ì•½ -> full_text)
    df['full_text'] = (
        df['ì¥ì '].fillna('').astype(str) + " " +
        df['ë‹¨ì '].fillna('').astype(str) + " " +
        df['ìš”ì•½'].fillna('').astype(str)
    )
    return df

# ========================================================================================
# [Inference Class] ì¶”ì²œ ì‹œìŠ¤í…œ
# ========================================================================================
class AIRecommender:
    def __init__(self, model_path, data_path):
        # 1. ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"[Inference] ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")

        # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')

        # 3. ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¡œë“œ ë° ê°€ì¤‘ì¹˜ ë³µì›
        try:
            # train ëª¨ë“ˆ ì˜ì¡´ì„± ì œê±° -> ë‚´ë¶€ í´ë˜ìŠ¤ ì‚¬ìš©
            self.model = ReviewRatingRegressor(n_targets=6)

            # [ìˆ˜ì •] ì½”ë© ê²½ë¡œ í˜¸í™˜ì„± ê°•í™”
            if not os.path.exists(model_path):
                 # ë§Œì•½ ì…ë ¥ëœ ê²½ë¡œì— ì—†ìœ¼ë©´ ì½”ë© ê¸°ë³¸ ê²½ë¡œ í™•ì¸
                if os.path.exists('/content/review_rating_model.pt'):
                    print(f"âš ï¸ ì§€ì •ëœ ê²½ë¡œ({model_path})ì— íŒŒì¼ì´ ì—†ì–´ '/content/review_rating_model.pt'ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    model_path = '/content/review_rating_model.pt'

            self.model.load_state_dict(torch.load(model_path, map_location=self.device))
            self.model.to(self.device)
            self.model.eval()
            print(f"âœ… í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (ê²½ë¡œ: {model_path})")

        except FileNotFoundError:
            raise FileNotFoundError(f"âŒ ëª¨ë¸ íŒŒì¼({model_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")

        # 4. ì¶”ì²œ í›„ë³´êµ° ë°ì´í„° ì¤€ë¹„
        self.data_path = data_path
        self.df = self._load_and_preprocess_data()

        # 5. ì „ì²´ ë°ì´í„° ì„ë² ë”© ìƒì„±
        print("\n[Inference] ì „ì²´ ê¸°ì—… ë°ì´í„°ì˜ ì„ë² ë”©(Vector)ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
        self.company_embeddings = self._get_embeddings(self.df['full_text'].tolist())
        print(f" -> ì„ë² ë”© ìƒì„± ì™„ë£Œ. (Shape: {self.company_embeddings.shape})")

    def _load_and_preprocess_data(self):
        # 1. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë° ê²½ë¡œ ë³´ì •
        if not os.path.exists(self.data_path):
            # ì½”ë© í™˜ê²½ ì˜ˆì™¸ ì²˜ë¦¬: ì—‘ì…€ íŒŒì¼ ìš°ì„  íƒìƒ‰
            if os.path.exists('/content/í†µí•©_ë¦¬ë·°_ê¸°ì—…ì¤‘ë³µì œê±°.xlsx'):
                 self.data_path = '/content/í†µí•©_ë¦¬ë·°_ê¸°ì—…ì¤‘ë³µì œê±°.xlsx'
            # CSV íŒŒì¼ (ë³€í™˜ëœ ê²ƒ) ì°¨ì„  íƒìƒ‰
            elif os.path.exists('/content/í†µí•©_ë¦¬ë·°_ê¸°ì—…ì¤‘ë³µì œê±°.xlsx - Sheet1.csv'):
                 self.data_path = '/content/í†µí•©_ë¦¬ë·°_ê¸°ì—…ì¤‘ë³µì œê±°.xlsx - Sheet1.csv'
            else:
                raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼({self.data_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        print(f"[Inference] ë°ì´í„° ë¡œë“œ ì¤‘... ({self.data_path})")

        # 2. í™•ì¥ìì— ë”°ë¥¸ ë¡œë“œ (Excel vs CSV)
        if self.data_path.endswith('.xlsx') or self.data_path.endswith('.xls'):
            try:
                df = pd.read_excel(self.data_path)
            except Exception as e:
                raise ValueError(f"ì—‘ì…€ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (openpyxl ì„¤ì¹˜ í™•ì¸ í•„ìš”): {e}")
        else:
            # CSVë¡œ ì‹œë„í•˜ë˜ ì‹¤íŒ¨í•˜ë©´ Excelë¡œ ì¬ì‹œë„ (í™•ì¥ìê°€ ì˜ëª»ëœ ê²½ìš° ëŒ€ë¹„)
            try:
                df = pd.read_csv(self.data_path)
            except:
                try:
                    df = pd.read_excel(self.data_path)
                except Exception as e:
                    raise ValueError(f"íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

        # ë‚´ë¶€ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì‚¬ìš©
        df = local_preprocess_dataframe(df)
        return df

    def _get_embeddings(self, text_list):
        embeddings = []
        batch_size = 32

        with torch.no_grad():
            for i in range(0, len(text_list), batch_size):
                batch_texts = text_list[i:i+batch_size]

                encoded = self.tokenizer.batch_encode_plus(
                    batch_texts,
                    max_length=128,
                    padding='max_length',
                    truncation=True,
                    return_tensors='pt'
                )

                input_ids = encoded['input_ids'].to(self.device)
                mask = encoded['attention_mask'].to(self.device)

                # ëª¨ë¸ í†µê³¼ -> ì„ë² ë”© ì¶”ì¶œ
                _, pooler_output = self.model(input_ids, mask)
                embeddings.append(pooler_output.cpu().numpy())

        return np.vstack(embeddings)

    def recommend(self, user_query, top_k=3):
        print(f"\n" + "="*60)
        print(f"ğŸ” ì‚¬ìš©ì ì…ë ¥: '{user_query}'")
        print("="*60)

        query_vec = self._get_embeddings([user_query])
        sim_scores = cosine_similarity(query_vec, self.company_embeddings).flatten()
        top_indices = sim_scores.argsort()[-top_k:][::-1]

        print(f"ğŸ¤– AI ì¶”ì²œ ê²°ê³¼ (Top {top_k}):")

        rank = 1
        for idx in top_indices:
            row = self.df.iloc[idx]
            sim = sim_scores[idx]

            c_name = row.get('company', 'Unknown')
            role = row.get('job_role', 'Unknown')
            rating = row.get('overall', 0.0)
            summary = str(row.get('ìš”ì•½', ''))[:50] + "..."

            print(f"\n[{rank}ìœ„] {c_name} ({role})")
            print(f"   â€¢ ì í•©ë„(ìœ ì‚¬ë„): {sim:.4f}")
            print(f"   â€¢ ì‹¤ì œ ì¢…í•©í‰ì : {rating}ì ")
            print(f"   â€¢ ë¦¬ë·° ìš”ì•½: {summary}")
            rank += 1

    def predict_score(self, text):
        encoded = self.tokenizer.encode_plus(
            text, max_length=128, padding='max_length', truncation=True, return_tensors='pt'
        )
        input_ids = encoded['input_ids'].to(self.device)
        mask = encoded['attention_mask'].to(self.device)

        with torch.no_grad():
            scores, _ = self.model(input_ids, mask)
            scores = scores.cpu().numpy()[0]

        print(f"\nğŸ“Š '{text[:20]}...'ì— ëŒ€í•œ AI í‰ì  ì˜ˆì¸¡:")
        labels = ['ë³µì§€/ê¸‰ì—¬', 'ì›Œë¼ë°¸', 'ë¬¸í™”', 'ê²½ì˜ì§„', 'ìŠ¹ì§„', 'ì¢…í•©']
        for label, score in zip(labels, scores):
            print(f"   - {label}: {score:.2f}ì ")

# ========================================================================================
# [Main Execution] ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
# ========================================================================================
if __name__ == "__main__":
    # ë°ì´í„° ê²½ë¡œ: ì½”ë©ì— ì—…ë¡œë“œëœ ì—‘ì…€ íŒŒì¼ ìš°ì„  (/content/...)
    target_data_path = '/content/í†µí•©_ë¦¬ë·°_ê¸°ì—…ì¤‘ë³µì œê±°.xlsx'

    # ëª¨ë¸ ê²½ë¡œ: ì½”ë©ì— ìƒì„±ëœ .pt íŒŒì¼ ê²½ë¡œ
    target_model_path = '/content/review_rating_model.pt'

    if not os.path.exists(target_data_path) and not os.path.exists('/content/í†µí•©_ë¦¬ë·°_ê¸°ì—…ì¤‘ë³µì œê±°.xlsx - Sheet1.csv'):
         print(f"âš ï¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '{target_data_path}' ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    try:
        rec_system = AIRecommender(
            model_path=target_model_path,
            data_path=target_data_path
        )

        # --- ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ---
        print("\nğŸ‘‡ [í…ŒìŠ¤íŠ¸ 1] ì›Œë¼ë°¸ ì¤‘ì‹œí˜•")
        rec_system.recommend("ì—°ë´‰ì€ ì ì–´ë„ ë˜ë‹ˆê¹Œ ì•¼ê·¼ ì—†ê³  ìˆ˜í‰ì ì¸ ê³³", top_k=3)

        print("\nğŸ‘‡ [í…ŒìŠ¤íŠ¸ 2] ì„±ì¥/ë³´ìƒ ì¤‘ì‹œí˜•")
        rec_system.recommend("ì¼ì´ í˜ë“¤ì–´ë„ ë°°ìš¸ê²Œ ë§ê³  ì„±ê³¼ê¸‰ í™•ì‹¤í•œ ê³³", top_k=3)

        # print("\nğŸ‘‡ [í…ŒìŠ¤íŠ¸ 3] ì ìˆ˜ ì˜ˆì¸¡")
        # rec_system.predict_score("ì•¼ê·¼ì´ ë§¤ì¼ ìˆê³  ì£¼ë§ ì¶œê·¼ë„ ê°•ìš”í•©ë‹ˆë‹¤. ì—°ë´‰ì€ ìµœì €ì„ê¸ˆ ìˆ˜ì¤€.")

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("1. í•™ìŠµ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ëª¨ë¸(pt íŒŒì¼)ì„ ìƒì„±í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        print("2. ì—‘ì…€ ë°ì´í„° íŒŒì¼(.xlsx)ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
